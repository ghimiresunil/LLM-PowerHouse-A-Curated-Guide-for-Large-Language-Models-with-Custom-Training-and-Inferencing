# Tutorials about LLM 

| Tutorials| Author |  Link |
|------ | ----------- | :--------- |
| State of GPT | Andrej Karpathy | [ğŸ”—](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2) | 
| Instruction finetuning and RLHF lecture | Hyung Won Chung | [ğŸ”—](https://www.youtube.com/watch?v=zjrM-MW-0y0)|
| Scaling, emergence, and reasoning in large language models | Jason Wei | [ğŸ”—](https://docs.google.com/presentation/d/1EUV7W7X_w0BDrscDhPg7lMGzJCkeaPkGCJ3bN8dluXc/edit?pli=1&resourcekey=0-7Nz5A7y8JozyVrnDtcEKJA#slide=id.g16197112905_0_0)|
| Open Pretrained Transformers | Susan Zhang | [ğŸ”—](https://www.youtube.com/watch?v=p9IxoSkvZ-M&t=4s)|
| How Does ChatGPT Work? | Ameet Deshpande | [ğŸ”—](https://docs.google.com/presentation/d/1TTyePrw-p_xxUbi3rbmBI3QQpSsTI1btaQuAUvvNc8w/edit#slide=id.g206fa25c94c_0_24)|
| GPT in 60 Lines of NumPy | Jay Mody | [ğŸ”—](https://jaykmody.com/blog/gpt-from-scratch/)|
| Welcome to the "Big Model" Era: Techniques and Systems to Train and Serve Bigger Models | ICML 2022 | [ğŸ”—](https://icml.cc/virtual/2022/tutorial/18440)|
| Foundational Robustness of Foundation Models | NeurIPS 2022 | [ğŸ”—](https://nips.cc/virtual/2022/tutorial/55796)|
| Let's build GPT: from scratch, in code, spelled out | Andrej Karpathy | [ğŸ”—](https://www.youtube.com/watch?v=kCc8FmEb1nY) [ğŸ‘¨â€ğŸ’»](https://github.com/karpathy/ng-video-lecture)|
| Prompt Engineering Guide | DAIR.AI | [ğŸ”—](https://github.com/dair-ai/Prompt-Engineering-Guide) |
| Fine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers | Philipp Schmid | [ğŸ”—](https://www.philschmid.de/fine-tune-flan-t5-deepspeed) |
| Illustrating Reinforcement Learning from Human Feedback (RLHF)  | HuggingFace | [ğŸ”—](https://huggingface.co/blog/rlhf) |
| What Makes a Dialog Agent Useful?  | HuggingFace | [ğŸ”—](https://huggingface.co/blog/dialog-agents) |
|  How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources | Yao Fu |  [ğŸ”—](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1) |
| What Is ChatGPT Doing â€¦ and Why Does It Work?  | Stephen Wolfram | [ğŸ”—](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) |
| Why did all of the public reproduction of GPT-3 fail?  | Jingfeng Yang | [ğŸ”—](https://jingfengyang.github.io/gpt) |
| Pure Rust implementation of a minimal Generative Pretrained Transformer  | Keyvan Kambakhsh | [ğŸ”—](https://github.com/keyvank/femtoGPT) |