# Overfitting And Underfitting In Deep Learning

Understanding underfitting and overfitting, the bane of deep learning, is crucial for building robust models. While frustrating at times, identifying these bugs ğŸ’© presents valuable learning opportunities. ğŸ”°

## What is Underfitting

Underfitting occurs when your model is too basic to grasp the intricacies of the training data. Imagine trying to distinguish between dogs and cats with just a single perceptron. It's like asking a blindfolded person to sort buttons by color!. 

Letâ€™s see the below image so you understand better.

![Underfitting](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/assets/40186859/3b0a1f59-86e1-4473-8797-affb536d1933)

Note âŒâ€” Underfitting is a simple term model donâ€™t learn features in training data.

## What Is Overfitting

Once you grasp underfitting, overfitting becomes a breezeâ€”it's like two sides of the same coin. Let's dive into the flip side!

Overfitting occurs when a model becomes overly complex and, instead of learning the underlying patterns in the training data, simply memorizes it. This leads to fantastic performance on the training set but disastrous performance on unseen data, like a student who aced a practice test but forgets everything on the real exam.

Letâ€™s see the below image so you understand better.

![Overfitting](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/assets/40186859/fa8c2f06-429b-420f-8576-f0c764f816b3)

I hope these explanations of underfitting and overfitting in deep learning have been clear. If not, fear not! I have a story in store that will make these concepts stick with you like glue.

## One Boy Two Thing Happen â€“ Underfitting & Overfitting

![One Boy Two Thing Happen](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/assets/40186859/e1c08f06-de9d-4ece-b030-eb23f431e7f3)

This story starts with one â€˜Studentâ€™ ğŸ‘¨ğŸ»â€ğŸ“ğŸ‘©ğŸ»â€ğŸ“ studying for an examğŸ’¯.

- Underfitting happens: When a student doesnâ€™t study enough so fails to the exam.ğŸ˜­
- Overfitting happens: 
While the student aced the exam by rote-learning the study material, their understanding remained shallow. When the teacher ventured beyond the textbook, the student's memorized answers proved insufficient, laying bare the gap between memorization and true comprehension.
    - Example: Paralleling rote memorization in exams, many in the deep learning field prioritize frameworks like [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), and [Jax](https://jax.readthedocs.io/en/latest/index.html) over grasping the core concepts. This leaves them vulnerable when new frameworks emerge, struggling to adapt due to their shallow understanding.

I hope you understand these two concepts at this moment. If not, donâ€™t worry one more time.

NoteğŸ”¥:
- If the model is doing very well on the training dataset but poorly on the validation set, it means â€˜Overfittingâ€™. Here is the example remember, if the training error is 1% and â€˜validaton_errorâ€™ 10%â€¦
- Or the same thing the opposite model performs badly on the training set itâ€™s called â€˜Underfittingâ€™. For example if the training error is 24% and validation error is 25%.

## One Image One Model Change The World

Crafting a model is a marathon, not a sprint, but the joy of progress fuels my journey. The knowledge that building a truly intelligent system is no easy feat, yet brimming with potential, keeps me eternally optimistic. In the following example, I'll unveil the kind of model that truly matters.

Overfiting and underfiting show this image and show good model performance in deep learning

![image](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/assets/40186859/62d90090-c753-4489-a051-7f3e0c9b2c81)





